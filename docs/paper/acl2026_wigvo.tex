\documentclass[11pt]{article}
\usepackage[review]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{fontspec}
\newfontfamily\koreanfont{AppleMyungjo}[Scale=MatchLowercase]
\newcommand{\ko}[1]{{\koreanfont #1}}

\title{WIGVO: Real-Time Bidirectional Speech Translation\\over Legacy PSTN Calls via Dual-Session Echo Gating}

\author{
  Hyeongseob Kim \\
  Soundmind Inc. \\
  \texttt{harrison@soundmind.co.kr}
}

\begin{document}
\maketitle

\begin{abstract}
Real-time speech translation with large language models (LLMs) has become feasible in controlled settings such as mobile apps and web browsers, where developers assume wideband audio and client-side echo cancellation. Recent end-to-end full-duplex models further push latency below 200\,ms but remain confined to broadband environments. However, deploying such systems over the Public Switched Telephone Network (PSTN) remains challenging due to narrowband G.711 audio, unpredictable round-trip delays, and absence of client-side signal processing. We present \textbf{WIGVO}, a server-side relay system that enables bidirectional LLM-based speech translation over ordinary telephone calls without requiring app installation or carrier integration. A central contribution is addressing what we term \emph{echo-induced self-reinforcing translation loops}: synthesized speech echoing back through the PSTN gets re-ingested and repeatedly translated. WIGVO solves this through a dual-session architecture with deterministic silence injection and energy-based voice activity detection (VAD) gating. We evaluate WIGVO on 97 completed PSTN calls (38 fully instrumented) across three communication modes---voice-to-voice (V2V), text-to-voice (T2V), and full-agent---observing 564\,ms median Session~A latency ($N{=}148$ turns), zero echo-induced translation loops, and USD\,0.42 per minute cost. WIGVO additionally implements a voice-to-text (VTT) accessibility mode for users with hearing impairments. The system is deployed at \url{https://wigvo.run} with a live demo and video walkthrough.
\end{abstract}

\section{Introduction}

Streaming speech-to-speech translation has advanced rapidly with Realtime LLM APIs, but existing work focuses on WebRTC and VoIP settings where wideband audio and client-side acoustic echo cancellation (AEC) are available \citep{seamless_streaming_2023,whisper_radford_2023}. The Public Switched Telephone Network (PSTN), however, still handles the majority of calls to restaurants, hospitals, and government offices \citep{itu_pstn_overview_2020}. PSTN operates at 8\,kHz with $\mu$-law companding, introduces 80--600\,ms jitter, and provides no client-side AEC. For foreign residents, people with speech anxiety, or users with hearing impairments, making such calls remains a significant barrier.

While recent full-duplex speech models such as Moshi~\citep{defossez2024moshi} and Hibiki~\citep{labiausse2025hibiki} achieve sub-200\,ms latency in broadband settings, they do not address PSTN-specific signal processing challenges.

We present \textbf{WIGVO}, a server-side relay that bridges a web client and a standard PSTN phone number through two concurrent LLM-backed streaming sessions. The caller speaks (or types) via a browser; the callee answers on an ordinary phone. WIGVO translates bidirectionally in real time, supporting voice-to-voice (V2V), text-to-voice (T2V), and full-agent modes, with an additional voice-to-text (VTT) accessibility mode implemented but not evaluated in this study.

A key challenge is what we call an \textbf{echo-induced self-reinforcing translation loop}: TTS audio played to the callee echoes back through the PSTN, is re-recognized as new speech, and gets translated again---creating a runaway feedback cycle. This problem is specific to full-duplex telephony channels and cannot be solved by VAD threshold tuning alone. Our solution operates at the architectural level: (i)~strict directional separation via two independent Realtime sessions, (ii)~deterministic silence injection during TTS playback windows, and (iii)~energy-based gating calibrated for $\mu$-law dynamics.

We evaluate WIGVO on 97 completed PSTN calls and report latency, echo suppression, and cost metrics. Our contributions are:
\begin{itemize}
    \item We formalize the \emph{echo-induced translation loop} problem in streaming speech translation over telephony.
    \item We propose a \emph{dual-session gated architecture} combining directional separation, silence injection, and energy-based gating for PSTN environments.
    \item We deploy and evaluate a working relay server across three communication modes, reporting system-level metrics from real PSTN calls.
\end{itemize}

\section{Related Work}

\paragraph{Simultaneous speech translation.}
Recent systems such as SeamlessM4T \citep{seamless_m4t_2023} and Seamless Streaming \citep{seamless_streaming_2023} achieve real-time speech translation with expressive and multilingual models. ESPnet-ST-v2 \citep{espnet_st_v2_2023} provides a comprehensive toolkit for offline and simultaneous translation. However, all assume wideband audio inputs from controlled environments and do not address PSTN-specific challenges such as G.711 codec artifacts, telephony echo, or narrowband VAD.

\paragraph{End-to-End Full-Duplex Speech Models.}
Recent work has moved beyond cascaded pipelines toward end-to-end full-duplex architectures that eliminate explicit turn boundaries. Moshi~\citep{defossez2024moshi} is the first real-time full-duplex spoken LLM, modeling user and system speech as parallel token streams with a practical latency of 200\,ms. PersonaPlex~\citep{roy2026personaplex} fine-tunes this architecture with hybrid voice and role conditioning, reducing smooth turn-taking latency to 70\,ms and surpassing commercial systems including Gemini Live on conversational naturalness. For speech-to-speech translation specifically, Hibiki~\citep{labiausse2025hibiki} applies the same multistream architecture to simultaneous interpretation, achieving near-human-interpreter quality on French-to-English translation in broadband settings. Notably, Hibiki's limitation to a single language pair underscores the difficulty of scaling end-to-end S2ST systems---a challenge that WIGVO sidesteps through its modular cascade design, which enables language extensibility by swapping individual components. However, all these systems assume clean, wideband (24\,kHz) audio. WIGVO addresses a complementary and underexplored setting: real-time S2ST over PSTN, where G.711 $\mu$-law encoding at 8\,kHz, acoustic echo, and codec-induced VAD failures require architectural intervention that end-to-end models do not provide.

\paragraph{Telephony AI agents.}
Google Duplex \citep{google_duplex_2018} demonstrated autonomous PSTN calls for restaurant reservations, but performs task completion in a single language rather than bidirectional translation. Commercial platforms such as Vapi and Bland.ai provide LLM-powered phone agents but focus on monolingual voice assistants.

\paragraph{Relay services for accessibility.}
Telecommunications Relay Services (TRS), mandated by the FCC in the United States, provide human operators who relay calls for deaf and hard-of-hearing users \citep{fcc_trs_2024}. These services are limited to specific countries, require human intermediaries, and do not support cross-lingual translation.

\paragraph{Accessibility-oriented translation.}
sign.mt \citep{sign_mt_2024} provides bidirectional sign language translation with an accessibility focus; our work targets telephony and addresses echo-induced loops. Table~\ref{tab:comparison} positions WIGVO against these systems.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{System} & \textbf{PSTN} & \textbf{Bidir.} & \textbf{S2S} & \textbf{Echo} & \textbf{A11y} \\
\midrule
SeamlessM4T & $\times$ & \checkmark & \checkmark & N/A & $\times$ \\
Google Duplex & \checkmark & $\times$ & $\times$ & Unk. & $\times$ \\
FCC TRS & \checkmark & \checkmark & $\times$ & Human & \checkmark \\
sign.mt & $\times$ & \checkmark & \checkmark & N/A & \checkmark \\
Vapi / Bland & \checkmark & $\times$ & $\times$ & Unk. & $\times$ \\
\textbf{WIGVO} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\caption{Comparison with existing systems. PSTN: works over telephone network; Bidir.: bidirectional translation; S2S: speech-to-speech; Echo: handles telephony echo; A11y: accessibility modes (T2V/VTT; VTT implemented, not evaluated).}
\label{tab:comparison}
\end{table}

\section{System Architecture}

Figure~\ref{fig:architecture} shows the high-level architecture. A browser-based client connects to the relay server via WebSocket, sending 16\,kHz PCM audio and receiving translated audio, captions, and status events. The relay manages two independent Realtime LLM sessions and a telephony gateway connection.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/architecture}
  \caption{WIGVO architecture. The relay server manages Session~A (caller$\to$callee) and Session~B (callee$\to$caller) as independent Realtime API connections, with a telephony gateway for PSTN audio.}
  \label{fig:architecture}
\end{figure}

\paragraph{AudioRouter and pipelines.}
An \texttt{AudioRouter} implements the Strategy pattern to dispatch events to one of four pipeline implementations: V2V, T2V, VTT, or Full-Agent (V2V, T2V, and Full-Agent are evaluated in this work). Each pipeline manages the lifecycle of two Realtime sessions and cross-cutting components (guardrails, context window, metrics).

\paragraph{Dual Realtime sessions.}
Session~A receives browser audio (PCM16) and produces translated G.711 $\mu$-law for the telephony gateway. Session~B receives PSTN audio (G.711 $\mu$-law, 8\,kHz) and produces translated audio and captions for the browser. Each session has its own system prompt, sliding context window (6 turns), and event stream, ensuring strict directional separation.

\paragraph{Communication modes.}
\textbf{V2V}: both directions use streaming ASR+TTS. \textbf{T2V}: the caller types text, which is converted to TTS for the callee; the callee's speech is transcribed and displayed as captions. \textbf{VTT}: the caller speaks; the callee receives text captions only.\footnote{VTT mode is fully implemented but excluded from quantitative evaluation due to insufficient call volume at submission time.} \textbf{Full-Agent}: the LLM autonomously conducts the call based on collected user intent.

\section{Echo Gating Mechanisms}

\subsection{The Echo Loop Problem}

In a naive single-session design, both speakers' audio feeds into one streaming session. TTS output played to the callee echoes back through the PSTN after 80--600\,ms, gets re-recognized, and triggers another translation cycle. We observed this reliably in early prototypes: the system would generate progressively distorted paraphrases of its own output until manually interrupted.

We initially attempted a Pearson-correlation echo detector comparing outgoing TTS buffers with incoming audio. While effective in controlled tests, it failed in production due to $\mu$-law nonlinear quantization, variable delays, and background noise producing spurious correlations. This detector is now disabled.

A single-session baseline without gating produced loops in the majority of test calls; a Pearson-correlation detector reduced but did not eliminate loops while introducing frequent false positives that blocked legitimate callee speech. Echo Gate v2 eliminated loops entirely while preserving callee interruption capability.

\subsection{Echo Gate v2: Silence Injection}

Our production solution, \textbf{Echo Gate v2}, operates deterministically:

\begin{enumerate}
    \item When Session~A begins streaming TTS to the PSTN, an \emph{echo window} is activated. Duration is estimated from byte length and sample rate, capped at 1.2\,s.
    \item During this window, incoming PSTN audio is replaced with $\mu$-law silence (0xFF) before forwarding to Session~B. This blocks echo at the server while allowing the Realtime API's VAD to naturally detect end-of-speech.
    \item A dynamic cooldown of 0.3\,s accounts for PSTN jitter before resuming normal forwarding.
\end{enumerate}

While comfort noise generation (RFC~3389; \citealt{rfc3389_cn_2002}) addresses perceptual continuity for human listeners in VoIP, we identify a distinct failure mode: echo-suppression-induced audio gaps cause streaming VAD state machine deadlock in neural speech APIs. Specifically, if the relay simply \emph{drops} audio during echo windows rather than injecting silence, the Realtime API's server-side VAD never observes a silence-to-speech transition and remains stuck in a ``speaking'' state indefinitely. To our knowledge, this failure mode and its mitigation via silence injection have not been reported in the context of neural streaming S2ST pipelines.

Across 97 completed calls, Echo Gate v2 produced zero echo-induced translation loops. Echo gate activated 136 times total (3.6/call on average), with 0 breakthrough incidents, confirming that silence injection effectively prevents self-reinforcing translation cycles.

\subsection{Energy-Based Gating}

RMS energy monitoring provides a secondary gate. During echo windows, only high-energy signals ($\geq$400 RMS) break through as genuine callee speech; typical echo energy (100--400 RMS) remains suppressed. Outside echo windows, a lower threshold (150 RMS) filters line noise without clipping normal speech. This two-tier scheme allows callee interruptions to override the echo gate immediately. A three-level interrupt priority (callee $>$ caller $>$ AI) ensures callee speech immediately cancels active TTS.


\section{PSTN-Aware VAD and Robustness}

\subsection{Server VAD Failure Modes on PSTN}

OpenAI's Realtime API provides a server-side VAD that works well for wideband WebRTC audio. On PSTN lines, however, we observed three failure modes:

\begin{enumerate}
    \item \textbf{Background noise misclassification.} Constant low-level PSTN noise (line hum, codec quantization artifacts) is classified as speech, causing the VAD to enter a ``speaking'' state that never terminates. We observed stuck durations ranging from 15\,s to 72\,s before manual intervention.
    \item \textbf{Delayed \texttt{speech\_stopped} events.} Even after genuine speech ends, the noisy PSTN floor prevents the energy level from dropping below the server VAD's silence threshold, delaying end-of-utterance detection by 15--72\,s.
    \item \textbf{Audio discontinuity deadlock.} When the relay drops audio frames (e.g., during echo suppression), the server VAD never observes a clean silence interval and fails to emit a \texttt{speech\_stopped} event entirely.
\end{enumerate}

\subsection{Two-Stage Local VAD}

To address these failures, WIGVO implements a two-stage local VAD on the relay server, applied to incoming PSTN audio before it reaches the Realtime API:

\paragraph{Stage 1: RMS Energy Gate.}
A fast energy check filters sub-speech signals. We set the threshold at 150~RMS based on empirical $\mu$-law noise distributions: background noise typically falls in the 50--200~RMS range, while speech aligns around 500--2{,}000+ RMS. Frames below 150~RMS are classified as noise and replaced with silence, preventing them from reaching Stage~2.

\paragraph{Stage 2: Silero VAD.}
Frames passing the energy gate are processed by Silero VAD \citep{silero_vad_2021}, a lightweight neural voice activity detector. Since Silero expects 16\,kHz input but PSTN audio arrives at 8\,kHz, we apply zero-order hold (ZOH) upsampling before inference. Each frame is 32\,ms (512 samples at 16\,kHz).

\paragraph{Hysteresis state machine.}
To avoid rapid state oscillation, we apply asymmetric hysteresis thresholds:

\begin{itemize}
    \item \textbf{SILENCE$\to$SPEAKING}: probability $\geq 0.5$ for 2 consecutive frames (64\,ms). Fast onset preserves responsiveness.
    \item \textbf{SPEAKING$\to$SILENCE}: probability $< 0.35$ for 15 consecutive frames (480\,ms). Slow offset avoids premature cutoff during natural pauses.
\end{itemize}

This design reduces \texttt{speech\_stopped} latency from 15--72\,s (server VAD on PSTN) to a consistent 480\,ms, a reduction of over 96\%.

\subsection{Silence Injection for VAD Continuity}

As noted in Section~4.2, the echo gate replaces PSTN audio with $\mu$-law silence (0xFF) rather than dropping frames. This design choice is critical for VAD continuity: if frames are dropped entirely, the Realtime API's streaming VAD loses its audio timeline and cannot detect speech boundaries. By injecting silence bytes, the relay maintains stream continuity while suppressing echo content. This is functionally distinct from comfort noise generation (RFC~3389; \citealt{rfc3389_cn_2002}), which synthesizes perceptually natural background noise for human listeners. Our silence injection instead targets the VAD state machine of neural speech APIs, preventing deadlock caused by audio gaps.

\subsection{STT Hallucination Blocklist}

A 15-pattern blocklist intercepts Korean broadcast-style hallucinations from Whisper on low-energy input; 5 outputs were blocked across 38 instrumented calls.

\section{Evaluation}
\label{sec:evaluation}

We evaluated WIGVO on 97 PSTN calls (38 fully instrumented) for Korean$\leftrightarrow$English translation.

\subsection{Latency}

Table~\ref{tab:latency} reports latency statistics across the 38 instrumented calls.

\begin{table}[t]
  \centering
  \footnotesize
  \begin{tabular}{lrrrrr}
    \toprule
    \textbf{Stage} & \textbf{Avg} & \textbf{P50} & \textbf{P95} & \textbf{Max} & \textbf{$N$} \\
    \midrule
    Session A (ms) & 618 & 564 & 1{,}023 & 2{,}256 & 148 \\
    Session B E2E (ms) & 2{,}270 & 2{,}023 & 5{,}378 & 10{,}503 & 214 \\
    \quad STT (ms) & 2{,}263 & 1{,}979 & 5{,}065 & --- & 196 \\
    \quad Translation (ms) & 757 & 464 & 2{,}209 & --- & 103 \\
    First message (ms) & 2{,}585 & 1{,}550 & 7{,}460 & 11{,}308 & 31 \\
    \bottomrule
  \end{tabular}
  \caption{Translation latency across 38 instrumented calls. Session~A: caller$\to$callee (ASR+translate+TTS). Session~B: callee$\to$caller, decomposed into STT (Whisper) and translation (GPT-4o) components. $N$: number of measured turns.}
  \label{tab:latency}
\end{table}

Session~A achieves 564\,ms median latency, within the range for interactive communication. Session~B, which depends on PSTN audio quality and streaming ASR, shows 2{,}023\,ms median. STT (Whisper) accounts for 74.7\% of Session~B mean latency across 103 paired turns, with translation contributing the remainder. Figure~\ref{fig:latency} shows the end-to-end latency distributions for Session~A and Session~B. Figure~\ref{fig:scatter} shows the correlation between utterance length and Session~B latency ($r=0.529$, $p<0.001$), STT accounts for 74.7\% of Session~B mean latency.

T2V mode shows lower Session~A latency (no caller-side ASR), while Agent mode shows higher Session~B latency due to function-calling overhead.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/figure2_latency_histogram}
  \caption{End-to-end latency distributions for Session~A (caller$\to$callee, $N{=}148$ turns) and Session~B (callee$\to$caller, $N{=}214$ turns) over live PSTN calls.}
  \label{fig:latency}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/figure3_utterance_scatter}
  \caption{Utterance length (characters) vs.\ Session~B E2E latency. Pearson $r=0.529$ ($p<0.001$), indicating that longer recipient utterances incur higher ASR-dominated latency.}
  \label{fig:scatter}
\end{figure}


\subsection{Echo and Safety}

Table~\ref{tab:safety} reports echo suppression, VAD, and guardrail statistics across the 38 instrumented calls.

\begin{table}[t]
  \centering
  \small
  \begin{tabular}{lrr}
    \toprule
    \textbf{Metric} & \textbf{Total} & \textbf{Per-call avg} \\
    \midrule
    Echo gate activations & 136 & 3.6 \\
    Echo-induced loops & 0 & 0.0 \\
    VAD false triggers & 62 & 1.6 \\
    Hallucinations blocked & 5 & 0.13 \\
    \bottomrule
  \end{tabular}
  \caption{Echo, VAD, and guardrail statistics over 38 instrumented calls. Zero echo loops were observed.}
  \label{tab:safety}
\end{table}

The echo gate activated 3.6 times per call on average, corresponding to TTS playback events. Critically, \textbf{zero echo-induced translation loops} were observed, whereas early prototypes without gating reliably produced such loops. VAD false triggers (1.6/call) were mitigated by a minimum speech duration filter (480\,ms hysteresis). The guardrail module blocked 5 hallucinated outputs (broadcast-style phrases from Whisper on low-energy input).


\subsection{Cost}

Across the 38 instrumented calls (45.5 minutes total), WIGVO consumed 638{,}035 tokens at a total cost of USD\,19.05, yielding \textbf{USD\,0.42/min} (USD\,0.50/call). These costs reflect dual-session Realtime API pricing and remain an order of magnitude cheaper than human relay services (USD\,1--3/min).

\section{Demonstration}

WIGVO is deployed at \url{https://wigvo.run} with a live demo accessible to reviewers.\footnote{Demo video: \url{https://youtu.be/PLACEHOLDER}} The web interface supports scenario selection (restaurant, hospital, government office), real-time caption display, and mode switching. Figure~\ref{fig:screenshot} shows the interface during an active V2V call.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/screenshot_call}
  \caption{WIGVO web interface during a V2V call. Left: chat and caption view with bidirectional translations. Right: call status, duration, and mode indicator.}
  \label{fig:screenshot}
\end{figure}

We demonstrate two scenarios:

\paragraph{Restaurant reservation (V2V).}
An English-speaking user calls a Korean restaurant. Session~A translates caller utterances into Korean TTS for the callee; Session~B returns Korean responses as English audio and captions. Across 38 instrumented calls, Session~A achieved 564\,ms median latency (P95: 1{,}023\,ms) and Session~B 2{,}023\,ms median (P95: 5{,}378\,ms; $r{=}0.529$ with utterance length, $p{<}0.001$).

\paragraph{Medical appointment (T2V).}
A Korean user with speech anxiety types messages that WIGVO speaks to a clinic via TTS; the clinic's spoken responses appear as real-time captions. A typing indicator is relayed to the callee during text composition.

\section{Discussion}

\paragraph{Scope of contribution.}
WIGVO's contribution is architectural: demonstrating that existing Realtime LLM APIs can be made robust for PSTN environments through dual-session separation, deterministic echo gating, and energy-based filtering. Translation quality is not independently evaluated; WIGVO inherits the translation capabilities of GPT-4o Realtime \citep{openai_realtime_api_2024}, which achieves state-of-the-art performance on Korean$\leftrightarrow$English translation. Formal translation quality evaluation (e.g., BLEU, chrF) is left for future work.

\paragraph{Broader applications.}
The dual-session gated architecture is applicable beyond translation: multilingual call centers, voice inclusion services for users with disabilities, and telephony-native LLM agents that combine task completion with cross-lingual communication.

\section{Limitations and Future Work}

\paragraph{Language Coverage.}
The current evaluation focuses on Korean-English telephone translation, a high-demand language pair for PSTN-based international communication. WIGVO's modular cascade architecture---STT via Whisper, translation via GPT-4o, TTS via the Realtime API---is inherently language-agnostic, and multilingual extension requires no architectural changes beyond model substitution. We leave systematic multilingual evaluation to future work, following the trajectory of Hibiki~\citep{labiausse2025hibiki}, which demonstrated that focused evaluation on a single language pair yields reproducible and meaningful benchmarks before broader scaling.

\paragraph{Broadband Environments.}
WIGVO targets PSTN constraints (G.711 $\mu$-law, 8\,kHz) by design. In broadband VoIP settings, the echo gate and ZOH upsampling components become unnecessary, and the system degrades gracefully to a standard cascade pipeline. Evaluating WIGVO in mixed PSTN-VoIP scenarios and benchmarking against end-to-end full-duplex models~\citep{defossez2024moshi, roy2026personaplex} in controlled conditions remains an open direction.

\paragraph{Toward Full-Duplex Translation.}
As production-ready full-duplex speech models mature~\citep{defossez2024moshi, labiausse2025hibiki}, the Session~B latency bottleneck in WIGVO---currently dominated by streaming ASR (median 1{,}939\,ms)---may be substantially reduced by replacing the STT stage with an end-to-end audio model. The PSTN relay layer, echo gate, and VAD architecture introduced in this work would remain necessary regardless of the underlying speech model, as they address network-level constraints orthogonal to model design.

\paragraph{Cost Optimization.}
Current per-minute cost varies by mode (V2V: \$0.25/min, T2V: \$0.39/min), driven primarily by OpenAI Realtime API audio token pricing. Future work includes exploring self-hosted STT alternatives (e.g., Whisper large-v3, Kyutai STT\footnote{\url{https://kyutai.org/stt}}) and lightweight TTS models to reduce operational cost while maintaining latency targets.

\paragraph{User Experience Evaluation.}
The current evaluation focuses on objective latency and system reliability metrics. A formal user study measuring perceived naturalness and task completion---such as a System Usability Scale (SUS) assessment---has not yet been conducted and remains planned as future work.

\paragraph{Echo Gate Threshold Tuning.}
The echo gate suppression window (currently fixed at 480\,ms post-TTS-completion) was determined empirically. An adaptive threshold mechanism that adjusts based on network round-trip time and observed acoustic conditions could reduce both false suppressions and breakthrough rates, and is left for future work.

\section*{Ethics Statement}

All calls were initiated by the authors and collaborators for system testing. Audio was logged for debugging and aggregate metrics only, with no personally identifiable information retained beyond temporary phone numbers. For deployment with real users, informed consent, data minimization, and compliance with privacy regulations (e.g., GDPR, PIPA) are required. The accessibility use cases (T2V for speech anxiety, VTT for hearing impairment) aim to reduce communication barriers but should be deployed with user agency and opt-in participation.

\section*{Acknowledgments}

We thank the anonymous reviewers for their feedback. WIGVO uses OpenAI's Realtime API and Twilio Media Streams. The relay server is implemented in Python with FastAPI.

\bibliography{references}
\bibliographystyle{acl_natbib}

\end{document}
